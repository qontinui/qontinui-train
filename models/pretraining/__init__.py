"""Self-supervised pre-training methods for GUI understanding.

This package contains implementations of various self-supervised learning
approaches designed for training vision models from scratch on large datasets:

Methods:
    - MAE (Masked Autoencoder): Masks patches and reconstructs them
    - Contrastive Learning: SimCLR, MoCo style instance discrimination
    - Multi-task Pre-training: Combines multiple pre-training objectives
"""
